#!/usr/bin/env python3
"""
å¿«é€Ÿæ¨¡å‹æµ‹è¯•ç¨‹åº - ä»…éªŒè¯æ¨¡å‹åŠ è½½ï¼Œä¸è¿›è¡Œå®é™…ç”Ÿæˆ
"""

import os
import torch
from datetime import datetime

def quick_test_stable_diffusion_xl():
    """å¿«é€Ÿæµ‹è¯• Stable Diffusion XL æ¨¡å‹åŠ è½½"""
    print("ğŸ¨ å¿«é€Ÿæµ‹è¯• Stable Diffusion XL")
    print("-" * 40)
    
    try:
        from diffusers import StableDiffusionXLPipeline
        
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        pipeline = StableDiffusionXLPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            variant="fp16" if torch.cuda.is_available() else None,
            use_safetensors=True
        )
        
        if torch.cuda.is_available():
            pipeline = pipeline.to("cuda")
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU")
        else:
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°CPU")
        
        # æ£€æŸ¥æ¨¡å‹ç»„ä»¶
        print(f"   UNet: {type(pipeline.unet).__name__}")
        print(f"   VAE: {type(pipeline.vae).__name__}")
        print(f"   æ–‡æœ¬ç¼–ç å™¨: {type(pipeline.text_encoder).__name__}")
        
        # æ¸…ç†å†…å­˜
        del pipeline
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def quick_test_stable_video_diffusion():
    """å¿«é€Ÿæµ‹è¯• Stable Video Diffusion æ¨¡å‹åŠ è½½"""
    print("\nğŸ¬ å¿«é€Ÿæµ‹è¯• Stable Video Diffusion")
    print("-" * 40)
    
    try:
        from diffusers import StableVideoDiffusionPipeline
        
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        pipeline = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            variant="fp16" if torch.cuda.is_available() else None
        )
        
        if torch.cuda.is_available():
            pipeline = pipeline.to("cuda")
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU")
        else:
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°CPU")
        
        # æ£€æŸ¥æ¨¡å‹ç»„ä»¶
        print(f"   UNet: {type(pipeline.unet).__name__}")
        print(f"   VAE: {type(pipeline.vae).__name__}")
        print(f"   å›¾åƒç¼–ç å™¨: {type(pipeline.image_encoder).__name__}")
        
        # æ¸…ç†å†…å­˜
        del pipeline
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def quick_test_speecht5():
    """å¿«é€Ÿæµ‹è¯• SpeechT5 æ¨¡å‹åŠ è½½"""
    print("\nğŸ¤ å¿«é€Ÿæµ‹è¯• SpeechT5")
    print("-" * 40)
    
    try:
        from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
        
        print("ğŸ”„ åŠ è½½å¤„ç†å™¨...")
        processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
        print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        print("ğŸ”„ åŠ è½½TTSæ¨¡å‹...")
        model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
        print("âœ… TTSæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        print("ğŸ”„ åŠ è½½å£°ç å™¨...")
        vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
        print("âœ… å£°ç å™¨åŠ è½½æˆåŠŸ")
        
        if torch.cuda.is_available():
            model = model.to("cuda")
            vocoder = vocoder.to("cuda")
            print("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
        else:
            print("âœ… æ¨¡å‹åœ¨CPUä¸Šè¿è¡Œ")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        print(f"   è¯æ±‡è¡¨å¤§å°: {model.config.vocab_size}")
        print(f"   éšè—å±‚å¤§å°: {model.config.hidden_size}")
        
        # æ¸…ç†å†…å­˜
        del processor, model, vocoder
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        print("\nğŸš€ GPUå†…å­˜ä¿¡æ¯")
        print("-" * 40)
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = props.total_memory / 1024**3
            
            print(f"GPU {i}: {props.name}")
            print(f"   æ€»å†…å­˜: {total:.1f} GB")
            print(f"   å·²åˆ†é…: {allocated:.1f} GB")
            print(f"   å·²ç¼“å­˜: {cached:.1f} GB")
            print(f"   å¯ç”¨: {total - cached:.1f} GB")
    else:
        print("\nâš ï¸ æœªæ£€æµ‹åˆ°CUDA GPU")

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½")
    print("-" * 40)
    
    try:
        # æµ‹è¯•torchåŸºæœ¬æ“ä½œ
        print("ğŸ”„ æµ‹è¯•PyTorchåŸºæœ¬æ“ä½œ...")
        x = torch.randn(2, 3)
        y = torch.randn(3, 4)
        z = torch.mm(x, y)
        print(f"âœ… çŸ©é˜µè¿ç®—: {z.shape}")
        
        # æµ‹è¯•CUDAæ“ä½œï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            print("ğŸ”„ æµ‹è¯•CUDAæ“ä½œ...")
            x_cuda = x.cuda()
            y_cuda = y.cuda()
            z_cuda = torch.mm(x_cuda, y_cuda)
            print(f"âœ… CUDAçŸ©é˜µè¿ç®—: {z_cuda.shape}")
        
        # æµ‹è¯•å›¾åƒå¤„ç†
        print("ğŸ”„ æµ‹è¯•å›¾åƒå¤„ç†...")
        from PIL import Image
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        img_array = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        img = Image.fromarray(img_array)
        print(f"âœ… å›¾åƒåˆ›å»º: {img.size}")
        
        # æµ‹è¯•éŸ³é¢‘å¤„ç†
        print("ğŸ”„ æµ‹è¯•éŸ³é¢‘å¤„ç†...")
        audio_data = np.random.randn(16000)  # 1ç§’çš„éŸ³é¢‘
        print(f"âœ… éŸ³é¢‘æ•°æ®: {audio_data.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ AIæ¨¡å‹å¿«é€Ÿæµ‹è¯•")
    print("=" * 60)
    
    start_time = datetime.now()
    
    # æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    print(f"ğŸ Pythonç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸš€ CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # æ£€æŸ¥GPUå†…å­˜
    check_gpu_memory()
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    basic_ok = test_basic_functionality()
    
    if not basic_ok:
        print("\nâŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢æ¨¡å‹æµ‹è¯•")
        return
    
    # æµ‹è¯•å„ä¸ªæ¨¡å‹
    results = {}
    
    print("\nğŸ¤– å¼€å§‹æ¨¡å‹åŠ è½½æµ‹è¯•")
    print("=" * 60)
    
    results['SDXL'] = quick_test_stable_diffusion_xl()
    results['SVD'] = quick_test_stable_video_diffusion()
    results['SpeechT5'] = quick_test_speecht5()
    
    # æœ€ç»ˆæ£€æŸ¥GPUå†…å­˜
    check_gpu_memory()
    
    # æ€»ç»“
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"â±ï¸  æµ‹è¯•è€—æ—¶: {duration:.1f} ç§’")
    print()
    
    for model, result in results.items():
        status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
        print(f"   {model}: {status}")
    
    all_passed = all(results.values())
    
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ’¡ å¯ä»¥è¿è¡Œå®Œæ•´çš„åŠŸèƒ½æµ‹è¯•")
    else:
        failed_models = [model for model, result in results.items() if not result]
        print(f"âš ï¸ ä»¥ä¸‹æ¨¡å‹åŠ è½½å¤±è´¥: {', '.join(failed_models)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½çŠ¶æ€")
    
    # ä¿å­˜å¿«é€Ÿæµ‹è¯•ç»“æœ
    try:
        os.makedirs("data/test_outputs", exist_ok=True)
        report_file = f"data/test_outputs/quick_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"AIæ¨¡å‹å¿«é€Ÿæµ‹è¯•æŠ¥å‘Š\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æµ‹è¯•è€—æ—¶: {duration:.1f} ç§’\n\n")
            f.write("æµ‹è¯•ç»“æœ:\n")
            for model, result in results.items():
                f.write(f"  {model}: {'æˆåŠŸ' if result else 'å¤±è´¥'}\n")
            f.write(f"\næ€»ä½“ç»“æœ: {'æ‰€æœ‰æµ‹è¯•é€šè¿‡' if all_passed else 'éƒ¨åˆ†æµ‹è¯•å¤±è´¥'}\n")
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
    except Exception as e:
        print(f"âš ï¸ æ— æ³•ä¿å­˜æµ‹è¯•æŠ¥å‘Š: {e}")

if __name__ == "__main__":
    main()