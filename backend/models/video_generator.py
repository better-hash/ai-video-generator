import os
import uuid
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from PIL import Image, ImageDraw, ImageFont
import numpy as np
from pathlib import Path
import torch
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Video:
    id: str
    file_path: str
    duration: float
    metadata: Dict[str, Any]

@dataclass
class VideoFrame:
    """è§†é¢‘å¸§æ•°æ®"""
    frame_number: int
    image_path: str
    timestamp: float
    characters: List[Dict[str, Any]]
    scene_description: str

class VideoGenerator:
    """è§†é¢‘ç”Ÿæˆå™¨ - é›†æˆAIæ¨¡å‹ç”ŸæˆçœŸå®è§†é¢‘"""
    
    def __init__(self):
        self.output_dir = "data/videos"
        self.temp_dir = "data/temp"
        self.fps = 24  # å¸§ç‡
        self.resolution = (1920, 1080)  # åˆ†è¾¨ç‡
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.output_dir, self.temp_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # åˆå§‹åŒ–AIæ¨¡å‹å±æ€§
        self.sd_pipeline = None
        self.svd_pipeline = None
        self.tts_pipeline = None
        
        # åˆå§‹åŒ–AIæ¨¡å‹ï¼ˆå®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Šï¼‰
        self._init_ai_models()
        
        print("ğŸ¬ è§†é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_ai_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        try:
            import torch
            from diffusers import StableVideoDiffusionPipeline, StableDiffusionXLPipeline
            from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
            
            print("ğŸ”„ æ­£åœ¨åŠ è½½AIæ¨¡å‹...")
            
            # è§†é¢‘ç”Ÿæˆæ¨¡å‹
            try:
                self.svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
                    "stabilityai/stable-video-diffusion-img2vid-xt",
                    torch_dtype=torch.float16,
                    variant="fp16"
                )
                if torch.cuda.is_available():
                    self.svd_pipeline = self.svd_pipeline.to("cuda")
                print("âœ… Stable Video Diffusion åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Stable Video Diffusion åŠ è½½å¤±è´¥: {e}")
                self.svd_pipeline = None
            
            # å›¾åƒç”Ÿæˆæ¨¡å‹ - ä½¿ç”¨SDXL
            try:
                self.sd_pipeline = StableDiffusionXLPipeline.from_pretrained(
                    "stabilityai/stable-diffusion-xl-base-1.0",
                    torch_dtype=torch.float16,
                    variant="fp16",
                    use_safetensors=True
                )
                if torch.cuda.is_available():
                    self.sd_pipeline = self.sd_pipeline.to("cuda")
                print("âœ… Stable Diffusion XL åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Stable Diffusion XL åŠ è½½å¤±è´¥: {e}")
                self.sd_pipeline = None
            
            # è¯­éŸ³åˆæˆæ¨¡å‹ - ä½¿ç”¨SpeechT5
            try:
                self.tts_processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
                self.tts_model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
                self.tts_vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
                
                if torch.cuda.is_available():
                    self.tts_model = self.tts_model.to("cuda")
                    self.tts_vocoder = self.tts_vocoder.to("cuda")
                
                # åˆ›å»ºé»˜è®¤è¯´è¯äººåµŒå…¥ - ä½¿ç”¨éšæœºåˆå§‹åŒ–è€Œä¸æ˜¯å…¨é›¶
                self.default_speaker_embedding = torch.randn(512) * 0.1
                if torch.cuda.is_available():
                    self.default_speaker_embedding = self.default_speaker_embedding.to("cuda")
                
                print("âœ… SpeechT5 TTS åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ SpeechT5 TTS åŠ è½½å¤±è´¥: {e}")
                self.tts_processor = None
                self.tts_model = None
                self.tts_vocoder = None
                self.default_speaker_embedding = None
            
            print("âœ… AIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ AIæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {e}")
            self.svd_pipeline = None
            self.sd_pipeline = None
            self.tts_processor = None
            self.tts_model = None
            self.tts_vocoder = None
            self.default_speaker_embedding = None
    
    def generate_video(self, script, characters: List, actions: List) -> Video:
        """ç”Ÿæˆå®Œæ•´è§†é¢‘"""
        try:
            video_id = str(uuid.uuid4())
            print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘: {video_id}")
            
            # 1. è§£æå‰§æœ¬ç»“æ„
            scenes = self._parse_script_to_scenes(script)
            
            # 2. ç”Ÿæˆåœºæ™¯èƒŒæ™¯
            scene_backgrounds = self._generate_scene_backgrounds(scenes)
            
            # 3. ç”Ÿæˆè§’è‰²å›¾åƒ
            character_images = self._generate_character_images(characters)
            
            # 4. ç”Ÿæˆè§†é¢‘å¸§åºåˆ—
            frames = self._generate_video_frames(scenes, scene_backgrounds, character_images, actions)
            
            # 5. åˆæˆæœ€ç»ˆè§†é¢‘
            video_path = self._compose_final_video(frames, video_id)
            
            # 6. ç”ŸæˆéŸ³é¢‘
            audio_path = self._generate_audio(script, video_id)
            
            # 7. åˆå¹¶éŸ³è§†é¢‘
            final_video_path = self._merge_audio_video(video_path, audio_path, video_id)
            
            # 8. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(frames, scene_backgrounds, character_images)
            
            print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {final_video_path}")
            
            return Video(
                id=video_id,
                file_path=final_video_path,
                duration=self._calculate_duration(frames),
                metadata={
                    "status": "completed",
                    "script": script.title if hasattr(script, 'title') else "unknown",
                    "scenes": len(scenes),
                    "characters": len(characters),
                    "frames": len(frames),
                    "resolution": self.resolution,
                    "fps": self.fps
                }
            )
            
        except Exception as e:
            print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return self._create_fallback_video(script, characters)
    
    def _parse_script_to_scenes(self, script) -> List[Dict[str, Any]]:
        """è§£æå‰§æœ¬ä¸ºåœºæ™¯åˆ—è¡¨"""
        scenes = []
        
        if hasattr(script, 'scenes') and script.scenes:
            for scene in script.scenes:
                scenes.append({
                    "id": scene.id,
                    "description": scene.description,
                    "duration": 5.0,  # é»˜è®¤5ç§’
                    "characters": scene.characters or [],
                    "actions": scene.actions or []
                })
        else:
            # å¦‚æœæ²¡æœ‰åœºæ™¯ä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤åœºæ™¯
            scenes.append({
                "id": "scene_001",
                "description": "é»˜è®¤åœºæ™¯",
                "duration": 10.0,
                "characters": [],
                "actions": []
            })
        
        return scenes
    
    def _generate_scene_backgrounds(self, scenes: List[Dict[str, Any]]) -> Dict[str, str]:
        """ç”Ÿæˆåœºæ™¯èƒŒæ™¯å›¾åƒ"""
        backgrounds = {}
        
        for scene in scenes:
            scene_id = scene["id"]
            description = scene["description"]
            
            if self.sd_pipeline:
                # ä½¿ç”¨AIæ¨¡å‹ç”ŸæˆèƒŒæ™¯
                background_path = self._generate_ai_background(description, scene_id)
            else:
                # ç”Ÿæˆå ä½èƒŒæ™¯
                background_path = self._generate_placeholder_background(description, scene_id)
            
            backgrounds[scene_id] = background_path
        
        return backgrounds
    
    def _generate_ai_background(self, description: str, scene_id: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹ç”ŸæˆèƒŒæ™¯"""
        try:
            # æ£€æŸ¥SDæ¨¡å‹æ˜¯å¦å¯ç”¨
            if not self.sd_pipeline:
                raise Exception("SDæ¨¡å‹æœªåŠ è½½")
                
            prompt = f"cinematic scene: {description}, high quality, detailed, professional photography"
            
            # ç”Ÿæˆå›¾åƒ
            result = self.sd_pipeline(
                prompt=prompt,
                num_inference_steps=30,
                guidance_scale=7.5,
            )
            
            # æ£€æŸ¥ç»“æœå¹¶è·å–å›¾åƒ
            image = None
            if hasattr(result, 'images') and result.images and len(result.images) > 0:
                image = result.images[0]
            elif hasattr(result, 'image'):
                image = result.image
            else:
                raise Exception(f"æ— æ•ˆçš„ç”Ÿæˆç»“æœæ ¼å¼: {type(result)}")
            
            # æ£€æŸ¥å›¾åƒç±»å‹
            if not hasattr(image, 'save'):
                raise Exception(f"ç”Ÿæˆçš„å›¾åƒç±»å‹æ— æ•ˆ: {type(image)}")
            
            # è°ƒæ•´å›¾åƒå¤§å°
            image = image.resize(self.resolution)
            
            # ä¿å­˜å›¾åƒ
            background_path = os.path.join(self.temp_dir, f"background_{scene_id}.png")
            image.save(background_path)
            
            print(f"âœ… èƒŒæ™¯å›¾åƒç”ŸæˆæˆåŠŸ: {background_path}")
            return background_path
            
        except Exception as e:
            print(f"âš ï¸ AIèƒŒæ™¯ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å ä½å›¾: {str(e)}")
            return self._generate_placeholder_background(description, scene_id)
    
    def _generate_placeholder_background(self, description: str, scene_id: str) -> str:
        """ç”Ÿæˆå ä½èƒŒæ™¯å›¾åƒ"""
        width, height = self.resolution
        
        # æ ¹æ®åœºæ™¯æè¿°é€‰æ‹©é¢œè‰²
        colors = {
            "é¤å…": (139, 69, 19),  # æ£•è‰²
            "å…¬å›­": (34, 139, 34),   # ç»¿è‰²
            "åŠå…¬å®¤": (105, 105, 105), # ç°è‰²
            "å®¶": (255, 228, 196),   # ç±³è‰²
            "è¡—é“": (128, 128, 128),  # ç°è‰²
        }
        
        color = (100, 150, 200)  # é»˜è®¤è“è‰²
        for keyword, rgb in colors.items():
            if keyword in description:
                color = rgb
                break
        
        # åˆ›å»ºèƒŒæ™¯å›¾åƒ
        image = Image.new('RGB', (width, height), color)
        draw = ImageDraw.Draw(image)
        
        # æ·»åŠ æ–‡å­—
        try:
            font = ImageFont.load_default()
        except:
            font = None
        
        text = f"åœºæ™¯èƒŒæ™¯\n{description}"
        text_bbox = draw.textbbox((0, 0), text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]
        
        x = (width - text_width) // 2
        y = (height - text_height) // 2
        
        draw.text((x, y), text, fill=(255, 255, 255), font=font)
        
        # ä¿å­˜å›¾åƒ
        background_path = os.path.join(self.temp_dir, f"background_{scene_id}.png")
        image.save(background_path)
        
        return background_path
    
    def _generate_character_images(self, characters: List) -> Dict[str, str]:
        """ç”Ÿæˆè§’è‰²å›¾åƒ"""
        character_images = {}
        
        for i, character in enumerate(characters):
            char_id = character.id if hasattr(character, 'id') else f"char_{i}"
            
            if self.sd_pipeline:
                # ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆè§’è‰²
                image_path = self._generate_ai_character(character, char_id)
            else:
                # ç”Ÿæˆå ä½è§’è‰²å›¾åƒ
                image_path = self._generate_placeholder_character(character, char_id)
            
            character_images[char_id] = image_path
        
        return character_images
    
    def _generate_ai_character(self, character, char_id: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆè§’è‰²å›¾åƒ"""
        try:
            # æ£€æŸ¥SDæ¨¡å‹æ˜¯å¦å¯ç”¨
            if not self.sd_pipeline:
                raise Exception("SDæ¨¡å‹æœªåŠ è½½")
                
            description = character.description if hasattr(character, 'description') else str(character)
            prompt = f"portrait of {description}, high quality, detailed face, professional photography"
            
            # ç”Ÿæˆå›¾åƒ
            result = self.sd_pipeline(
                prompt=prompt,
                num_inference_steps=30,
                guidance_scale=7.5,
            )
            
            # æ£€æŸ¥ç»“æœå¹¶è·å–å›¾åƒ
            image = None
            if hasattr(result, 'images') and result.images and len(result.images) > 0:
                image = result.images[0]
            elif hasattr(result, 'image'):
                image = result.image
            else:
                raise Exception(f"æ— æ•ˆçš„ç”Ÿæˆç»“æœæ ¼å¼: {type(result)}")
            
            # æ£€æŸ¥å›¾åƒç±»å‹
            if not hasattr(image, 'save'):
                raise Exception(f"ç”Ÿæˆçš„å›¾åƒç±»å‹æ— æ•ˆ: {type(image)}")
            
            # ä¿å­˜å›¾åƒ
            image_path = os.path.join(self.temp_dir, f"character_{char_id}.png")
            image.save(image_path)
            
            print(f"âœ… è§’è‰²å›¾åƒç”ŸæˆæˆåŠŸ: {image_path}")
            return image_path
            
        except Exception as e:
            print(f"âš ï¸ AIè§’è‰²ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å ä½å›¾: {e}")
            return self._generate_placeholder_character(character, char_id)
    
    def _generate_placeholder_character(self, character, char_id: str) -> str:
        """ç”Ÿæˆå ä½è§’è‰²å›¾åƒ"""
        width, height = 512, 512
        
        # æ ¹æ®è§’è‰²æè¿°é€‰æ‹©é¢œè‰²
        description = character.description if hasattr(character, 'description') else str(character)
        
        colors = {
            "ç”·": (100, 150, 200),  # è“è‰²
            "å¥³": (200, 150, 200),  # ç´«è‰²
            "å¹´è½»": (150, 200, 150), # ç»¿è‰²
            "è€å¹´": (200, 150, 150), # çº¢è‰²
        }
        
        color = (128, 128, 128)  # é»˜è®¤ç°è‰²
        for keyword, rgb in colors.items():
            if keyword in description:
                color = rgb
                break
        
        # åˆ›å»ºè§’è‰²å›¾åƒ
        image = Image.new('RGB', (width, height), color)
        draw = ImageDraw.Draw(image)
        
        # æ·»åŠ æ–‡å­—
        try:
            font = ImageFont.load_default()
        except:
            font = None
        
        text = f"è§’è‰²å›¾åƒ\n{description[:30]}..."
        text_bbox = draw.textbbox((0, 0), text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]
        
        x = (width - text_width) // 2
        y = (height - text_height) // 2
        
        draw.text((x, y), text, fill=(255, 255, 255), font=font)
        
        # ä¿å­˜å›¾åƒ
        image_path = os.path.join(self.temp_dir, f"character_{char_id}.png")
        image.save(image_path)
        
        return image_path
    
    def _generate_video_frames(self, scenes: List[Dict], backgrounds: Dict[str, str], 
                              characters: Dict[str, str], actions: List) -> List[VideoFrame]:
        """ç”Ÿæˆè§†é¢‘å¸§åºåˆ—"""
        frames = []
        frame_number = 0
        
        for scene in scenes:
            scene_id = scene["id"]
            background_path = backgrounds[scene_id]
            duration = scene["duration"]
            
            # è®¡ç®—è¯¥åœºæ™¯çš„å¸§æ•°
            scene_frames = int(duration * self.fps)
            
            for i in range(scene_frames):
                timestamp = frame_number / self.fps
                
                # ç”Ÿæˆå¸§å›¾åƒ
                frame_path = self._generate_frame_image(
                    background_path, characters, scene, actions, frame_number
                )
                
                frame = VideoFrame(
                    frame_number=frame_number,
                    image_path=frame_path,
                    timestamp=timestamp,
                    characters=list(characters.keys()),
                    scene_description=scene["description"]
                )
                
                frames.append(frame)
                frame_number += 1
        
        return frames
    
    def _generate_frame_image(self, background_path: str, characters: Dict[str, str], 
                             scene: Dict, actions: List, frame_number: int) -> str:
        """ç”Ÿæˆå•å¸§å›¾åƒ"""
        # åŠ è½½èƒŒæ™¯
        background = Image.open(background_path).resize(self.resolution)
        
        # åˆæˆè§’è‰²åˆ°èƒŒæ™¯ä¸Š
        composite_image = background.copy()
        draw = ImageDraw.Draw(composite_image)
        
        # ç®€å•çš„è§’è‰²å¸ƒå±€ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å¸ƒå±€ç®—æ³•ï¼‰
        char_positions = self._calculate_character_positions(len(characters), self.resolution)
        
        # åˆæˆè§’è‰²å›¾åƒ
        for i, (char_id, char_image_path) in enumerate(characters.items()):
            if i < len(char_positions) and os.path.exists(char_image_path):
                try:
                    # åŠ è½½è§’è‰²å›¾åƒ
                    char_image = Image.open(char_image_path).resize((200, 200))
                    
                    # è®¡ç®—ä½ç½®
                    x, y = char_positions[i]
                    
                    # åˆæˆåˆ°èƒŒæ™¯ä¸Š
                    composite_image.paste(char_image, (x, y), char_image if char_image.mode == 'RGBA' else None)
                    
                    # æ·»åŠ è§’è‰²åç§°æ ‡ç­¾
                    char_name = char_id.replace('char_', 'è§’è‰²')
                    try:
                        font = ImageFont.load_default()
                    except:
                        font = None
                    
                    # åœ¨è§’è‰²ä¸‹æ–¹æ·»åŠ åç§°
                    text_bbox = draw.textbbox((0, 0), char_name, font=font)
                    text_width = text_bbox[2] - text_bbox[0]
                    text_x = x + 100 - text_width // 2
                    text_y = y + 220
                    
                    # æ·»åŠ æ–‡å­—èƒŒæ™¯
                    draw.rectangle([text_x-5, text_y-5, text_x+text_width+5, text_y+20], 
                                 fill=(0, 0, 0, 128))
                    draw.text((text_x, text_y), char_name, fill=(255, 255, 255), font=font)
                    
                except Exception as e:
                    print(f"âš ï¸ è§’è‰²å›¾åƒåˆæˆå¤±è´¥: {e}")
        
        # æ·»åŠ å°è¯å­—å¹•ï¼ˆå¦‚æœæœ‰å¯¹è¯ï¼‰
        if hasattr(scene, 'dialogues') and scene.get('dialogues'):
            dialogue = scene['dialogues'][0] if scene['dialogues'] else None
            if dialogue:
                try:
                    font = ImageFont.load_default()
                except:
                    font = None
                
                # åœ¨åº•éƒ¨æ·»åŠ å­—å¹•
                subtitle_text = f"{dialogue.get('character', 'è§’è‰²')}: {dialogue.get('content', 'å°è¯')}"
                text_bbox = draw.textbbox((0, 0), subtitle_text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_x = (self.resolution[0] - text_width) // 2
                text_y = self.resolution[1] - 80
                
                # æ·»åŠ å­—å¹•èƒŒæ™¯
                draw.rectangle([text_x-10, text_y-10, text_x+text_width+10, text_y+30], 
                             fill=(0, 0, 0, 180))
                draw.text((text_x, text_y), subtitle_text, fill=(255, 255, 255), font=font)
        
        # ä¿å­˜å¸§
        frame_path = os.path.join(self.temp_dir, f"frame_{frame_number:06d}.png")
        composite_image.save(frame_path)
        
        return frame_path
    
    def _calculate_character_positions(self, num_characters: int, resolution: tuple) -> List[tuple]:
        """è®¡ç®—è§’è‰²ä½ç½®"""
        width, height = resolution
        
        if num_characters == 1:
            return [(width // 2 - 100, height // 2 - 100)]
        elif num_characters == 2:
            return [
                (width // 3 - 100, height // 2 - 100),
                (2 * width // 3 - 100, height // 2 - 100)
            ]
        else:
            # æ›´å¤šè§’è‰²çš„ç½‘æ ¼å¸ƒå±€
            positions = []
            cols = int(np.ceil(np.sqrt(num_characters)))
            rows = int(np.ceil(num_characters / cols))
            
            for i in range(num_characters):
                row = i // cols
                col = i % cols
                x = (col + 1) * width // (cols + 1) - 100
                y = (row + 1) * height // (rows + 1) - 100
                positions.append((x, y))
            
            return positions
    
    def _compose_final_video(self, frames: List[VideoFrame], video_id: str) -> str:
        """åˆæˆæœ€ç»ˆè§†é¢‘"""
        try:
            import cv2
            
            # è·å–ç¬¬ä¸€å¸§çš„å°ºå¯¸
            first_frame = cv2.imread(frames[0].image_path)
            height, width, layers = first_frame.shape
            
            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            video_path = os.path.join(self.output_dir, f"{video_id}_temp.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_path, fourcc, self.fps, (width, height))
            
            # å†™å…¥å¸§
            for frame in frames:
                img = cv2.imread(frame.image_path)
                out.write(img)
            
            out.release()
            return video_path
            
        except ImportError:
            print("âš ï¸ OpenCVæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè§†é¢‘")
            return self._create_simulation_video(frames, video_id)
        except Exception as e:
            print(f"âš ï¸ è§†é¢‘åˆæˆå¤±è´¥: {e}")
            return self._create_simulation_video(frames, video_id)
    
    def _create_simulation_video(self, frames: List[VideoFrame], video_id: str) -> str:
        """åˆ›å»ºæ¨¡æ‹Ÿè§†é¢‘æ–‡ä»¶"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ–‡ä»¶ä½œä¸ºè§†é¢‘å ä½ç¬¦
        video_path = os.path.join(self.output_dir, f"{video_id}_temp.txt")
        
        with open(video_path, 'w', encoding='utf-8') as f:
            f.write(f"æ¨¡æ‹Ÿè§†é¢‘æ–‡ä»¶ - {video_id}\n")
            f.write(f"æ€»å¸§æ•°: {len(frames)}\n")
            f.write(f"å¸§ç‡: {self.fps}\n")
            f.write(f"åˆ†è¾¨ç‡: {self.resolution}\n")
            f.write(f"æ—¶é•¿: {len(frames) / self.fps:.2f}ç§’\n")
        
        return video_path
    
    def _generate_audio(self, script, video_id: str) -> str:
        """ç”ŸæˆéŸ³é¢‘"""
        try:
            if self.tts_processor and self.tts_model and self.tts_vocoder:
                # ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆè¯­éŸ³
                return self._generate_ai_audio(script, video_id)
            else:
                # ç”Ÿæˆå ä½éŸ³é¢‘
                return self._generate_placeholder_audio(script, video_id)
                
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_placeholder_audio(script, video_id)
    
    def _generate_ai_audio(self, script, video_id: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹ç”ŸæˆéŸ³é¢‘"""
        try:
            # æ£€æŸ¥TTSæ¨¡å‹æ˜¯å¦å¯ç”¨
            if not (self.tts_processor and self.tts_model and self.tts_vocoder and self.default_speaker_embedding is not None):
                raise Exception("TTSæ¨¡å‹æˆ–speaker_embeddingæœªåŠ è½½")
            
            # æå–å¯¹è¯æ–‡æœ¬
            dialogues = []
            if hasattr(script, 'dialogues') and script.dialogues:
                for dialogue in script.dialogues:
                    dialogues.append(f"{dialogue.character}: {dialogue.content}")
            
            if not dialogues:
                dialogues = ["æ¬¢è¿è§‚çœ‹AIç”Ÿæˆçš„è§†é¢‘"]
            
            # åˆå¹¶æ‰€æœ‰å¯¹è¯ï¼ˆé™åˆ¶é•¿åº¦é¿å…è¿‡é•¿ï¼‰
            full_text = " ".join(dialogues)[:200]  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            
            # ç”Ÿæˆè¯­éŸ³
            audio_path = os.path.join(self.temp_dir, f"audio_{video_id}.wav")
            
            # ä½¿ç”¨SpeechT5ç”Ÿæˆè¯­éŸ³
            inputs = self.tts_processor(text=full_text, return_tensors="pt")
            
            # ç§»åŠ¨è¾“å…¥åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available() and self.tts_model.device.type == 'cuda':
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # ç¡®ä¿speaker_embeddingç»´åº¦æ­£ç¡®
            speaker_embedding = self.default_speaker_embedding
            if speaker_embedding.dim() == 1:
                speaker_embedding = speaker_embedding.unsqueeze(0)
            
            # ç”Ÿæˆè¯­éŸ³
            with torch.no_grad():
                speech = self.tts_model.generate_speech(
                    inputs["input_ids"], 
                    speaker_embedding, 
                    vocoder=self.tts_vocoder
                )
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            try:
                import soundfile as sf
                # ç¡®ä¿éŸ³é¢‘æ•°æ®åœ¨CPUä¸Š
                audio_data = speech.cpu().numpy() if hasattr(speech, 'cpu') else speech
                sf.write(audio_path, audio_data, 16000)
            except ImportError:
                # å¦‚æœsoundfileä¸å¯ç”¨ï¼Œåˆ›å»ºå ä½æ–‡ä»¶
                raise Exception("soundfileåº“æœªå®‰è£…")
            
            print(f"âœ… AIéŸ³é¢‘ç”ŸæˆæˆåŠŸ: {audio_path}")
            return audio_path
                
        except Exception as e:
            print(f"âš ï¸ AIéŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_placeholder_audio(script, video_id)
    
    def _generate_placeholder_audio(self, script, video_id: str) -> str:
        """ç”Ÿæˆå ä½éŸ³é¢‘æ–‡ä»¶"""
        audio_path = os.path.join(self.temp_dir, f"audio_{video_id}.txt")
        
        with open(audio_path, 'w', encoding='utf-8') as f:
            f.write(f"æ¨¡æ‹ŸéŸ³é¢‘æ–‡ä»¶ - {video_id}\n")
            f.write("è¿™é‡Œåº”è¯¥æ˜¯åˆæˆçš„è¯­éŸ³å†…å®¹\n")
        
        return audio_path
    
    def _merge_audio_video(self, video_path: str, audio_path: str, video_id: str) -> str:
        """åˆå¹¶éŸ³è§†é¢‘"""
        try:
            import cv2
            
            # ç®€å•çš„éŸ³è§†é¢‘åˆå¹¶ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰
            final_path = os.path.join(self.output_dir, f"{video_id}.mp4")
            
            # å¤åˆ¶è§†é¢‘æ–‡ä»¶
            import shutil
            shutil.copy2(video_path, final_path)
            
            return final_path
            
        except Exception as e:
            print(f"âš ï¸ éŸ³è§†é¢‘åˆå¹¶å¤±è´¥: {e}")
            # è¿”å›è§†é¢‘æ–‡ä»¶è·¯å¾„
            return video_path
    
    def _calculate_duration(self, frames: List[VideoFrame]) -> float:
        """è®¡ç®—è§†é¢‘æ—¶é•¿"""
        return len(frames) / self.fps
    
    def _cleanup_temp_files(self, frames: List[VideoFrame], backgrounds: Dict[str, str], 
                           characters: Dict[str, str]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            # åˆ é™¤å¸§å›¾åƒ
            for frame in frames:
                if os.path.exists(frame.image_path):
                    os.remove(frame.image_path)
            
            # åˆ é™¤èƒŒæ™¯å›¾åƒ
            for background_path in backgrounds.values():
                if os.path.exists(background_path):
                    os.remove(background_path)
            
            # åˆ é™¤è§’è‰²å›¾åƒ
            for char_path in characters.values():
                if os.path.exists(char_path):
                    os.remove(char_path)
                    
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def _create_fallback_video(self, script, characters: List) -> Video:
        """åˆ›å»ºå¤‡ç”¨è§†é¢‘ï¼ˆå½“ç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""
        video_id = str(uuid.uuid4())
        fallback_path = os.path.join(self.output_dir, f"{video_id}_fallback.txt")
        
        with open(fallback_path, 'w', encoding='utf-8') as f:
            f.write("è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¿™æ˜¯å¤‡ç”¨æ–‡ä»¶\n")
            f.write(f"å‰§æœ¬: {getattr(script, 'title', 'unknown')}\n")
            f.write(f"è§’è‰²æ•°: {len(characters)}\n")
        
        return Video(
            id=video_id,
            file_path=fallback_path,
            duration=10.0,
            metadata={"status": "fallback", "error": "ç”Ÿæˆå¤±è´¥"}
        ) 

class VideoGenerator:
    """ä½¿ç”¨ Stable Video Diffusion ç”Ÿæˆè§†é¢‘"""
    
    def __init__(self, model_id: str = "stabilityai/stable-video-diffusion"):
        """
        åˆå§‹åŒ– Stable Video Diffusion æ¨¡å‹
        
        :param model_id: Hugging Face æ¨¡å‹ ID
        """
        self.output_dir = "data/videos"
        os.makedirs(self.output_dir, exist_ok=True)
        
        try:
            from diffusers import StableVideoDiffusionPipeline
            
            self.pipe = StableVideoDiffusionPipeline.from_pretrained(
                model_id, 
                torch_dtype=torch.float16,
                variant="fp16"
            )
            
            # å¦‚æœæœ‰ GPUï¼Œç§»åŠ¨æ¨¡å‹åˆ° GPU
            if torch.cuda.is_available():
                self.pipe = self.pipe.to("cuda")
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.pipe = None
    
    def generate_video(self, image_path: str, duration: float = 4.0) -> Optional[str]:
        """
        ä»å›¾åƒç”Ÿæˆè§†é¢‘
        
        :param image_path: è¾“å…¥å›¾åƒè·¯å¾„
        :param duration: è§†é¢‘æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        :return: ç”Ÿæˆçš„è§†é¢‘è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        if not self.pipe:
            print("è§†é¢‘ç”Ÿæˆæ¨¡å‹æœªåˆå§‹åŒ–")
            return None
        
        try:
            # ç”Ÿæˆè§†é¢‘ IDï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            video_id = str(int(os.times().elapsed))
            video_path = os.path.join(self.output_dir, f"{video_id}.mp4")
            
            # ç”Ÿæˆè§†é¢‘
            video = self.pipe(
                image_path, 
                num_frames=int(duration * 8),  # å‡è®¾ 8 fps
                num_inference_steps=50,
                decode_chunk_size=8
            )
            
            # ä¿å­˜è§†é¢‘
            video[0].save(video_path)
            
            return video_path
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return None